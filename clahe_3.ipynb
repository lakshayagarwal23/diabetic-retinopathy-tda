{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47675541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "# [1] Setup and Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from skimage import exposure\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "import gc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import gudhi as gd\n",
    "\n",
    "# Paths/configs\n",
    "WORKSPACE_ROOT = os.getcwd()\n",
    "IMAGES_DIR = os.path.join(WORKSPACE_ROOT, 'train_images')\n",
    "LABELS_CSV = os.path.join(WORKSPACE_ROOT, 'train.csv')\n",
    "OUTPUT_DIR = os.path.join(WORKSPACE_ROOT, 'preprocessed_images_full_balanced')\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "RESIZE_SHAPE = (512, 512)\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93ac4223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3662 valid images.\n",
      "Class distribution (orig):\n",
      "  0 (No DR): 1805\n",
      "  1 (Mild): 370\n",
      "  2 (Moderate): 999\n",
      "  3 (Severe): 193\n",
      "  4 (Proliferative): 295\n",
      "Imbalance ratio: 9.35:1\n"
     ]
    }
   ],
   "source": [
    "labels_df = pd.read_csv(LABELS_CSV)\n",
    "img_files = set(os.listdir(IMAGES_DIR))\n",
    "processing_list = []\n",
    "for _, row in labels_df.iterrows():\n",
    "    fname = f\"{row['id_code']}.png\"\n",
    "    if fname in img_files:\n",
    "        processing_list.append({'id_code': row['id_code'],\n",
    "                               'diagnosis': int(row['diagnosis']),\n",
    "                               'image_path': os.path.join(IMAGES_DIR, fname)})\n",
    "print(f\"Found {len(processing_list)} valid images.\")\n",
    "\n",
    "# Class Count\n",
    "dist = labels_df['diagnosis'].value_counts().sort_index()\n",
    "class_names = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferative']\n",
    "print(\"Class distribution (orig):\")\n",
    "for i, cnt in enumerate(dist): print(f\"  {i} ({class_names[i]}): {cnt}\")\n",
    "imbalance_ratio = dist.max() / max(1, dist.min())\n",
    "print(f\"Imbalance ratio: {imbalance_ratio:.2f}:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8677ca5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced per class: 1805 Total after balance: 9025\n"
     ]
    }
   ],
   "source": [
    "# Augmentation helpers\n",
    "def augment_image(img, augmentation_type):\n",
    "    if augmentation_type == 'rotation':\n",
    "        angle = random.uniform(-15, 15)\n",
    "        from skimage.transform import rotate\n",
    "        return rotate(img, angle, preserve_range=True)\n",
    "    elif augmentation_type == 'flip_horizontal':\n",
    "        return np.fliplr(img)\n",
    "    elif augmentation_type == 'flip_vertical':\n",
    "        return np.flipud(img)\n",
    "    elif augmentation_type == 'gaussian_noise':\n",
    "        noise = np.random.normal(0, 0.02, img.shape)\n",
    "        return np.clip(img + noise, 0, 1)\n",
    "    elif augmentation_type == 'brightness':\n",
    "        factor = random.uniform(0.8, 1.2)\n",
    "        return np.clip(img * factor, 0, 1)\n",
    "    elif augmentation_type == 'contrast':\n",
    "        factor = random.uniform(0.8, 1.2)\n",
    "        mean = img.mean()\n",
    "        return np.clip((img - mean) * factor + mean, 0, 1)\n",
    "    else:\n",
    "        return img\n",
    "\n",
    "full_class_groups = {}\n",
    "for item in processing_list:\n",
    "    full_class_groups.setdefault(item['diagnosis'], []).append(item)\n",
    "full_target = max(len(items) for items in full_class_groups.values())\n",
    "augmentation_types = ['rotation','flip_horizontal','flip_vertical','gaussian_noise','brightness','contrast']\n",
    "balanced_list = []\n",
    "for class_id, items in full_class_groups.items():\n",
    "    balanced_list.extend(items)\n",
    "    need = full_target - len(items)\n",
    "    for i in range(need):\n",
    "        orig = random.choice(items)\n",
    "        aug_item = orig.copy()\n",
    "        aug_item['id_code'] = f\"{orig['id_code']}_aug_{i}\"\n",
    "        aug_item['is_augmented'] = True\n",
    "        aug_item['augmentation_type'] = random.choice(augmentation_types)\n",
    "        balanced_list.append(aug_item)\n",
    "print('Balanced per class:', full_target, 'Total after balance:', len(balanced_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69d3936f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conservative success: 1.00\n",
      "Moderate success: 1.00\n",
      "Current success: 1.00\n",
      "Aggressive success: 1.00\n",
      "Best CLAHE config: {'clip_limit': 0.01, 'tile_grid_size': (4, 4), 'name': 'Conservative'}\n"
     ]
    }
   ],
   "source": [
    "# [4] CLAHE Parameter Tuning (Sampled)\n",
    "# Use default params as fallback (tune using a sample for speed)\n",
    "CLAHE_CONFIGS = [\n",
    "    {'clip_limit': 0.01, 'tile_grid_size': (4,4), 'name': 'Conservative'},\n",
    "    {'clip_limit': 0.02, 'tile_grid_size': (8,8), 'name': 'Moderate'},\n",
    "    {'clip_limit': 0.03, 'tile_grid_size': (8,8), 'name': 'Current'},\n",
    "    {'clip_limit': 0.05, 'tile_grid_size': (16,16), 'name': 'Aggressive'}\n",
    "]\n",
    "def apply_clahe_preprocessing(img, clip_limit=0.03, tile_grid_size=(8,8)):\n",
    "    img_uint8 = (np.clip(img,0,1)*255).astype(np.uint8)\n",
    "    return exposure.equalize_adapthist(img_uint8, clip_limit=clip_limit, nbins=256).astype(np.float32)\n",
    "\n",
    "def process_single_image(image_info, resize_shape, clahe_params):\n",
    "    img = imread(image_info['image_path']).astype(np.float32)/255.0\n",
    "    if image_info.get('is_augmented',False):\n",
    "        img = augment_image(img, image_info['augmentation_type'])\n",
    "    if resize_shape is not None:\n",
    "        img = resize(img, resize_shape, anti_aliasing=True, preserve_range=True)\n",
    "    green = img[...,1] if img.ndim == 3 else img\n",
    "    green_clahe = apply_clahe_preprocessing(green, **{k:v for k,v in clahe_params.items() if k!='name'})\n",
    "    return green_clahe\n",
    "\n",
    "sample_set = random.sample(balanced_list, min(100, len(balanced_list)))\n",
    "best_param, best_acc = None, 0\n",
    "for config in CLAHE_CONFIGS:\n",
    "    ok = 0\n",
    "    for entry in sample_set:\n",
    "        try:\n",
    "            output = process_single_image(entry, RESIZE_SHAPE, config)\n",
    "            ok += int(output is not None and output.shape == RESIZE_SHAPE)\n",
    "        except: pass\n",
    "    acc = ok / len(sample_set)\n",
    "    print(f\"{config['name']} success: {acc:.2f}\")\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_param = config\n",
    "print(\"Best CLAHE config:\", best_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37139d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLAHE batches: 100%|██████████| 181/181 [31:58<00:00, 10.60s/it]\n"
     ]
    }
   ],
   "source": [
    "%pip install tqdm\n",
    "from tqdm import tqdm  # pyright: ignore[reportMissingModuleSource]\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "for bi in tqdm(range(0, len(balanced_list), BATCH_SIZE), desc='CLAHE batches'):\n",
    "    batch = balanced_list[bi:bi+BATCH_SIZE]\n",
    "    for item in batch:\n",
    "        try:\n",
    "            outimg = process_single_image(item, RESIZE_SHAPE, best_param)\n",
    "            mask = np.ones_like(outimg, dtype=bool)  # Optionally: apply circular mask as in your advanced version\n",
    "            vals = outimg[mask]\n",
    "            mu, sigma = (vals.mean(), vals.std()) if vals.size else (0, 1)\n",
    "            normalized = np.zeros_like(outimg)\n",
    "            normalized[mask] = (outimg[mask] - mu) / max(sigma, 1e-6)\n",
    "            np.save(os.path.join(OUTPUT_DIR, f\"{item['id_code']}_clahe_full_balanced.npy\"), normalized)\n",
    "        except Exception as e:\n",
    "            print(f\"Error ({item['id_code']}):\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f82803c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TDA feature extraction: 100%|██████████| 9025/9025 [35:54<00:00,  4.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix: (9025, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_barcodes(img2d):\n",
    "    img2d = np.max(img2d) - img2d\n",
    "    cc = gd.CubicalComplex(dimensions=img2d.shape, top_dimensional_cells=img2d.flatten())\n",
    "    cc.persistence()\n",
    "    H0 = np.array(cc.persistence_intervals_in_dimension(0))\n",
    "    H1 = np.array(cc.persistence_intervals_in_dimension(1))\n",
    "    return H0, H1\n",
    "\n",
    "def summarize_barcode(barcode):\n",
    "    if not len(barcode): return [0]*6\n",
    "    lifetimes = barcode[:,1]-barcode[:,0]\n",
    "    return [len(lifetimes), np.mean(lifetimes), np.max(lifetimes), np.sum(lifetimes),\n",
    "            np.mean(barcode[:,0]), np.mean(barcode[:,1])]\n",
    "\n",
    "processed_files = sorted(f for f in os.listdir(OUTPUT_DIR) if f.endswith('.npy'))\n",
    "X, y = [], []\n",
    "for fname in tqdm(processed_files, desc=\"TDA feature extraction\"):\n",
    "    img = np.load(os.path.join(OUTPUT_DIR,fname)).astype(np.float32)\n",
    "    H0,H1 = compute_barcodes(img)\n",
    "    X.append(summarize_barcode(H0)+summarize_barcode(H1))\n",
    "    id_code = fname.replace('_clahe_full_balanced.npy','').split('_aug_')[0]\n",
    "    label = labels_df[labels_df['id_code']==id_code]['diagnosis'].values[0]\n",
    "    y.append(label)\n",
    "X, y = np.array(X), np.array(y)\n",
    "print('Feature matrix:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d44e8a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN count: 0\n",
      "Inf count: 36100\n",
      "Fold 1 acc: 0.4199445983379501\n",
      "Fold 2 acc: 0.42049861495844876\n",
      "Fold 3 acc: 0.4149584487534626\n",
      "Fold 4 acc: 0.4038781163434903\n",
      "Fold 5 acc: 0.4105263157894737\n",
      "\n",
      "=== Final FULL DATASET Metrics ===\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        No DR       0.56      0.65      0.60      1805\n",
      "         Mild       0.44      0.57      0.50      1805\n",
      "     Moderate       0.38      0.34      0.35      1805\n",
      "       Severe       0.34      0.31      0.32      1805\n",
      "Proliferative       0.28      0.21      0.24      1805\n",
      "\n",
      "     accuracy                           0.41      9025\n",
      "    macro avg       0.40      0.41      0.40      9025\n",
      " weighted avg       0.40      0.41      0.40      9025\n",
      "\n",
      "Confusion matrix:\n",
      " [[1166   95   95  220  229]\n",
      " [ 226 1029  174  198  178]\n",
      " [ 226  383  605  353  238]\n",
      " [ 233  379  305  555  333]\n",
      " [ 229  465  428  302  381]]\n",
      "Overall accuracy: 0.414\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Check for problematic values\n",
    "print(\"NaN count:\", np.isnan(X).sum())\n",
    "print(\"Inf count:\", np.isinf(X).sum())\n",
    "\n",
    "# Replace infinities and NaNs with finite values (e.g., column mean or 0)\n",
    "X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Continue as before\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "preds = np.zeros_like(y)\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(X_scaled, y), 1):\n",
    "    model = SVC(kernel='linear', class_weight='balanced', random_state=42)\n",
    "    model.fit(X_scaled[train_idx], y[train_idx])\n",
    "    y_pred = model.predict(X_scaled[test_idx])\n",
    "    preds[test_idx] = y_pred\n",
    "    print(f\"Fold {fold} acc:\", accuracy_score(y[test_idx], y_pred))\n",
    "\n",
    "print(\"\\n=== Final FULL DATASET Metrics ===\")\n",
    "print(classification_report(y, preds, labels=[0, 1, 2, 3, 4], target_names=class_names))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y, preds, labels=[0, 1, 2, 3, 4]))\n",
    "print(f\"Overall accuracy: {accuracy_score(y, preds):.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
