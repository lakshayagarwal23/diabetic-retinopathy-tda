{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries \n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.models as models\n",
    "from gudhi import CubicalComplex\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "DATA_DIR_TRAIN = \"train_images/train_images\"\n",
    "DATA_DIR_VAL   = \"val_images/val_images\"\n",
    "DATA_DIR_TEST  = \"test_images/test_images\"\n",
    "\n",
    "CSV_TRAIN = \"train_1.csv\"\n",
    "CSV_VAL   = \"valid.csv\"\n",
    "CSV_TEST  = \"test.csv\"\n",
    "\n",
    "IMG_SIZE = (256, 256)\n",
    "TDA_OUT_SIZE = (64, 64)\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 30\n",
    "PATIENCE = 7\n",
    "LR = 1e-4\n",
    "RANDOM_SEED = 50\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "LAMBDA_MASK = 1.0\n",
    "PRECOMPUTE_TDA = False\n",
    "TDA_DIR = \"./tda_cache_fixed\"\n",
    "NUM_WORKERS = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproducibility\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess & TDA Functions\n",
    "def preprocess_green_clahe(img_rgb: Image.Image, resize_shape=IMG_SIZE):\n",
    "    \"\"\"Return normalized green-channel image (float32) for TDA computation.\"\"\"\n",
    "    img = img_rgb.resize(resize_shape)\n",
    "    arr = np.asarray(img).astype(np.float32) / 255.0\n",
    "    green = arr[..., 1]\n",
    "    g8 = (np.clip(green, 0, 1) * 255).astype(np.uint8)\n",
    "    green_eq = np.array(ImageOps.equalize(Image.fromarray(g8)), dtype=np.float32) / 255.0\n",
    "\n",
    "    #Mask \n",
    "    h, w = green_eq.shape\n",
    "    cy, cx = h // 2, w // 2\n",
    "    r = int(0.9 * min(cy, cx))\n",
    "    Y, X = np.ogrid[:h, :w]\n",
    "    mask = (X - cx) ** 2 + (Y - cy) ** 2 <= r ** 2\n",
    "\n",
    "    #Mean and Std \n",
    "    vals = green_eq[mask]\n",
    "    mu, sigma = (float(vals.mean()), float(vals.std())) if vals.size > 0 else (0.0, 1.0)\n",
    "    sigma = sigma if sigma > 1e-6 else 1.0\n",
    "\n",
    "    norm = np.zeros_like(green_eq, dtype=np.float32)\n",
    "    norm[mask] = (green_eq[mask] - mu) / sigma\n",
    "    \n",
    "    # Scale norm to [0,1] for better stability in persistence diagram\n",
    "    if norm.max() != norm.min():\n",
    "        nmin, nmax = norm.min(), norm.max()\n",
    "        norm = (norm - nmin) / (nmax - nmin + 1e-8)\n",
    "    return norm\n",
    "\n",
    "def compute_persistence_diagram_from_array(a2d: np.ndarray):\n",
    "    \"\"\"Compute 0/1-dim persistence intervals using CubicalComplex on -a2d.\"\"\"\n",
    "    a2d = np.asarray(a2d, dtype=np.float32)\n",
    "    top_cells = (-a2d).ravel()\n",
    "    try:\n",
    "        cc = CubicalComplex(dimensions=a2d.shape, top_dimensional_cells=top_cells)\n",
    "        cc.persistence()\n",
    "        pd0 = np.array(cc.persistence_intervals_in_dimension(0))\n",
    "        pd1 = np.array(cc.persistence_intervals_in_dimension(1))\n",
    "    except Exception:\n",
    "        pd0 = np.zeros((0, 2), dtype=np.float32)\n",
    "        pd1 = np.zeros((0, 2), dtype=np.float32)\n",
    "    return pd0, pd1\n",
    "\n",
    "def persistence_image_from_diagram(diag, out_size=(64, 64), sigma=0.03):\n",
    "    \"\"\"Convert PD (N,2) to a persistence image in [0,1] range.\"\"\"\n",
    "    H, W = out_size\n",
    "    img = np.zeros((H, W), dtype=np.float32)\n",
    "    if diag is None or len(diag) == 0:\n",
    "        return img\n",
    "    births, deaths = diag[:, 0], diag[:, 1]\n",
    "    finite_deaths = np.where(np.isinf(deaths), births + 1.0, deaths)\n",
    "    pers = finite_deaths - births\n",
    "    if len(births) == 0 or len(pers) == 0:\n",
    "        return img\n",
    "    b_min, b_max = births.min(), births.max() + 1e-8\n",
    "    p_min, p_max = pers.min(), pers.max() + 1e-8\n",
    "    births_n = (births - b_min) / (b_max - b_min + 1e-8)\n",
    "    pers_n = (pers - p_min) / (p_max - p_min + 1e-8)\n",
    "    ys = np.linspace(0, 1, H)\n",
    "    xs = np.linspace(0, 1, W)\n",
    "    Xg, Yg = np.meshgrid(xs, ys)\n",
    "    for b, p in zip(births_n, pers_n):\n",
    "        d2 = (Xg - b) ** 2 + (Yg - p) ** 2\n",
    "        img += np.exp(-0.5 * d2 / (sigma ** 2))\n",
    "    if img.max() > 0:\n",
    "        img /= img.max()\n",
    "    return img.astype(np.float32)\n",
    "\n",
    "def compute_persistence_image_from_green(green_norm, out_size=TDA_OUT_SIZE):\n",
    "    pd0, pd1 = compute_persistence_diagram_from_array(green_norm)\n",
    "    diag = pd1 if len(pd1) > 0 else pd0\n",
    "    return persistence_image_from_diagram(diag, out_size=out_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ DATASET ------------------\n",
    "class AptosTDA_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    For each sample:\n",
    "    - Load RGB image\n",
    "    - Compute or load persistence image (single channel float32)\n",
    "    - Apply random flip & rotation with SAME params to both RGB and persistence image\n",
    "    - Return normalized image tensor and persistence tensor (1xHxW)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, data_dir, img_size=IMG_SIZE,\n",
    "                 compute_tda_on_the_fly=True, tda_out_size=TDA_OUT_SIZE,\n",
    "                 precompute_dir=None, transforms=None):\n",
    "        \n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.data_dir = data_dir\n",
    "        self.img_size = img_size\n",
    "        self.compute_tda_on_the_fly = compute_tda_on_the_fly\n",
    "        self.tda_out_size = tda_out_size\n",
    "        self.precompute_dir = precompute_dir\n",
    "        self.transforms = transforms  # only normalization / ToTensor expected here\n",
    "\n",
    "        if (not self.compute_tda_on_the_fly) and (self.precompute_dir is None):\n",
    "            raise ValueError(\"If not computing on the fly, you must provide precompute_dir\")\n",
    "\n",
    "        if self.precompute_dir and not os.path.exists(self.precompute_dir):\n",
    "            os.makedirs(self.precompute_dir, exist_ok=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def resolve_image_path(self, id_code):\n",
    "        for ext in [\".png\", \".jpg\", \".jpeg\"]:\n",
    "            p = os.path.join(self.data_dir, id_code + ext)\n",
    "            if os.path.exists(p):\n",
    "                return p\n",
    "        return None\n",
    "\n",
    "    def maybe_load_precomputed(self, idc):\n",
    "        fname = os.path.join(self.precompute_dir, f\"{idc}.npz\")\n",
    "        if os.path.exists(fname):\n",
    "            try:\n",
    "                arr = np.load(fname)[\"tda_img\"].astype(np.float32)\n",
    "                return arr\n",
    "            except Exception:\n",
    "                return None\n",
    "        return None\n",
    "\n",
    "    def precompute_and_save(self, idc, green_norm):\n",
    "        arr = compute_persistence_image_from_green(green_norm, out_size=self.tda_out_size)\n",
    "        fname = os.path.join(self.precompute_dir, f\"{idc}.npz\")\n",
    "        np.savez_compressed(fname, tda_img=arr)\n",
    "        return arr\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        idc, label = row['id_code'], int(row['diagnosis'])\n",
    "        p = self.resolve_image_path(idc)\n",
    "        if p:\n",
    "            pil = Image.open(p).convert(\"RGB\").resize(self.img_size)\n",
    "        else:\n",
    "            pil = Image.new(\"RGB\", self.img_size)\n",
    "\n",
    "        # compute / load persistence image based on green-channel preprocessing\n",
    "        if self.compute_tda_on_the_fly:\n",
    "            green_norm = preprocess_green_clahe(pil, resize_shape=self.img_size)\n",
    "            tda_img = compute_persistence_image_from_green(green_norm, out_size=self.tda_out_size)\n",
    "        else:\n",
    "            loaded = self.maybe_load_precomputed(idc)\n",
    "            if loaded is None:\n",
    "                green_norm = preprocess_green_clahe(pil, resize_shape=self.img_size)\n",
    "                tda_img = self.precompute_and_save(idc, green_norm)\n",
    "            else:\n",
    "                tda_img = loaded\n",
    "\n",
    "        # convert to PIL for geometric transforms to keep same ops on both\n",
    "        tda_pil = Image.fromarray((tda_img * 255).astype(np.uint8)).convert(\"L\").resize(self.img_size)\n",
    "\n",
    "        # apply identical random geometric transforms to both\n",
    "        # Random horizontal flip\n",
    "        if random.random() < 0.5:\n",
    "            pil = TF.hflip(pil)\n",
    "            tda_pil = TF.hflip(tda_pil)\n",
    "\n",
    "        # Random rotation between -10 and 10 degrees\n",
    "        angle = random.uniform(-10, 10)\n",
    "        pil = TF.rotate(pil, angle, interpolation=InterpolationMode.BILINEAR)\n",
    "        tda_pil = TF.rotate(tda_pil, angle, interpolation=InterpolationMode.BILINEAR)\n",
    "\n",
    "        #tda -> tensor float32 [0,1]\n",
    "        if self.transforms is not None:\n",
    "            img_tensor = self.transforms(pil)\n",
    "        else:\n",
    "            img_tensor = TF.to_tensor(pil)\n",
    "\n",
    "        tda_tensor = TF.to_tensor(tda_pil).float() / 1.0  # already in 0-1 after ToTensor\n",
    "\n",
    "        tda_tensor = tda_tensor.float()\n",
    "\n",
    "        return img_tensor, tda_tensor, label, idc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ MODEL ------------------\n",
    "class CNNBackboneSpatial(nn.Module):\n",
    "    def __init__(self, pretrained=True, freeze_backbone=False):\n",
    "        super().__init__()\n",
    "        from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "        weights = EfficientNet_B0_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "        net = efficientnet_b0(weights=weights)\n",
    "\n",
    "        # Keep only feature extractor\n",
    "        self.stem = net.features\n",
    "\n",
    "        if freeze_backbone:\n",
    "            for p in self.stem.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.stem(x)\n",
    "\n",
    "class TopologyGuidedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements exactly: maxpool(Fi) + avgpool(Fi) + topo_resized -> Conv -> Sigmoid\n",
    "    Returns gated features Fi_att and attention map (1 x H x W)\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super().__init__()\n",
    "        pad = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(1, 1, kernel_size=kernel_size, padding=pad)\n",
    "\n",
    "    def forward(self, Fi, pimg):\n",
    "        \"\"\"\n",
    "        Fi: [B, C, Hf, Wf]\n",
    "        pimg: [B, 1, Horig, Worig] (will be resized)\n",
    "        \"\"\"\n",
    "        # resize topo mask to feature map spatial dims\n",
    "        p_resized = F.interpolate(pimg, size=Fi.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        # channel-wise max and avg pool -> [B,1,Hf,Wf]\n",
    "        max_pool = torch.max(Fi, dim=1, keepdim=True)[0]\n",
    "        avg_pool = torch.mean(Fi, dim=1, keepdim=True)\n",
    "\n",
    "        combined = max_pool + avg_pool + p_resized  # element-wise sum as in diagram\n",
    "        att_logits = self.conv(combined)\n",
    "        att = torch.sigmoid(att_logits)\n",
    "        Fi_att = Fi * att\n",
    "        return Fi_att, att, p_resized\n",
    "\n",
    "class TDA_CNN_Attention_Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Backbone -> TopologyGuidedAttention -> post conv -> classifier\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "\n",
    "        # infer backbone feature channels by a dummy forward\n",
    "        dummy = torch.zeros(1, 3, IMG_SIZE[1], IMG_SIZE[0])\n",
    "        with torch.no_grad():\n",
    "            feat = self.backbone(dummy)\n",
    "        _, C, _, _ = feat.shape\n",
    "\n",
    "        self.att_module = TopologyGuidedAttention(kernel_size=7)\n",
    "\n",
    "        # post-attention conv then global pool -> fc\n",
    "        self.post_conv = nn.Sequential(\n",
    "            nn.Conv2d(C, max(32, C // 2), kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.fc = nn.Linear(max(32, C // 2), num_classes)\n",
    "\n",
    "    def forward(self, x_rgb, pimg):\n",
    "        Feat = self.backbone(x_rgb)                       # [B, C, Hf, Wf]\n",
    "        Feat_att, att_map, p_resized = self.att_module(Feat, pimg)\n",
    "        z = self.post_conv(Feat_att).flatten(1)\n",
    "        logits = self.fc(z)\n",
    "        return logits, att_map, p_resized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=0.4):\n",
    "    \"\"\"Returns mixed inputs, pairs of targets, and lambda.\"\"\"\n",
    "    lam = np.random.beta(alpha, alpha) if alpha > 0 else 1\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "# ------------------ TRAIN / VAL ------------------\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion_ce, lambda_mask, device):\n",
    "    model.train()\n",
    "    total, correct, running_loss, running_ce, running_mask = 0, 0, 0, 0, 0\n",
    "    for imgs, tda_imgs, labels, _ in tqdm(dataloader, desc=\"Train\"):\n",
    "        imgs, tda_imgs, labels = imgs.to(device), tda_imgs.to(device), labels.to(device).long()\n",
    "        optimizer.zero_grad()\n",
    "        # ---- MIXUP ----\n",
    "        imgs_mix, y_a, y_b, lam = mixup_data(imgs, labels, alpha=0.4)\n",
    "        logits, att_pred, pimg_resized = model(imgs_mix, tda_imgs)\n",
    "\n",
    "        ce_loss = lam * criterion_ce(logits, y_a) + (1 - lam) * criterion_ce(logits, y_b)\n",
    "        mask_loss = F.mse_loss(att_pred, pimg_resized)\n",
    "\n",
    "        loss = ce_loss + lambda_mask * mask_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        running_ce += ce_loss.item() * imgs.size(0)\n",
    "        running_mask += mask_loss.item() * imgs.size(0)\n",
    "        _, preds = logits.max(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += imgs.size(0)\n",
    "    return running_loss/total, running_ce/total, running_mask/total, correct/total\n",
    "\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    O = confusion_matrix(y_true, y_pred, labels=[0,1,2,3,4])\n",
    "    N = O.sum()\n",
    "\n",
    "    w = np.zeros((5,5))\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            w[i,j] = ((i - j)**2) / ((5 - 1)**2)\n",
    "\n",
    "    act_hist = np.sum(O, axis=1)\n",
    "    pred_hist = np.sum(O, axis=0)\n",
    "    E = np.outer(act_hist, pred_hist) / N\n",
    "\n",
    "    kappa = 1 - (np.sum(w * O) / np.sum(w * E))\n",
    "    return kappa\n",
    "\n",
    "def validate_one_epoch(model, dataloader, criterion_ce, lambda_mask, device):\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    running_loss = running_ce = running_mask = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, tda_imgs, labels, _ in dataloader:\n",
    "            imgs, tda_imgs, labels = imgs.to(device), tda_imgs.to(device), labels.to(device).long()\n",
    "            logits, att_pred, pimg_resized = model(imgs, tda_imgs)\n",
    "\n",
    "            ce_loss = criterion_ce(logits, labels)\n",
    "            mask_loss = F.mse_loss(att_pred, pimg_resized)\n",
    "            loss = ce_loss + lambda_mask * mask_loss\n",
    "\n",
    "            running_loss += loss.item() * imgs.size(0)\n",
    "            running_ce += ce_loss.item() * imgs.size(0)\n",
    "            running_mask += mask_loss.item() * imgs.size(0)\n",
    "\n",
    "            _, preds = logits.max(1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += imgs.size(0)\n",
    "\n",
    "    acc = correct / total\n",
    "    qwk = quadratic_weighted_kappa(all_labels, all_preds)\n",
    "\n",
    "    return running_loss/total, running_ce/total, running_mask/total, acc, qwk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ TEST EVALUATION ------------------\n",
    "def evaluate_test(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    preds_all, labels_all = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, tda_imgs, labels, _ in dataloader:\n",
    "            imgs, tda_imgs, labels = imgs.to(device), tda_imgs.to(device), labels.to(device).long()\n",
    "            logits, _, _ = model(imgs, tda_imgs)\n",
    "            _, preds = logits.max(1)\n",
    "\n",
    "            preds_all.extend(preds.cpu().tolist())\n",
    "            labels_all.extend(labels.cpu().tolist())\n",
    "\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += imgs.size(0)\n",
    "\n",
    "    acc = correct / total\n",
    "    qwk = quadratic_weighted_kappa(labels_all, preds_all)\n",
    "    return acc, qwk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ MAIN ------------------\n",
    "def main():\n",
    "    # Load three CSVs directly\n",
    "    train_df = pd.read_csv(CSV_TRAIN)[['id_code', 'diagnosis']]\n",
    "    val_df   = pd.read_csv(CSV_VAL)[['id_code', 'diagnosis']]\n",
    "    test_df  = pd.read_csv(CSV_TEST)[['id_code', 'diagnosis']]\n",
    "\n",
    "    # Basic transforms\n",
    "    normalize = T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    transforms = T.Compose([T.ToTensor(), normalize])\n",
    "\n",
    "    # Datasets\n",
    "    ds_train = AptosTDA_Dataset(train_df, DATA_DIR_TRAIN, img_size=IMG_SIZE,\n",
    "                                compute_tda_on_the_fly=(not PRECOMPUTE_TDA),\n",
    "                                tda_out_size=TDA_OUT_SIZE, precompute_dir=TDA_DIR,\n",
    "                                transforms=transforms)\n",
    "\n",
    "    ds_val = AptosTDA_Dataset(val_df, DATA_DIR_VAL, img_size=IMG_SIZE,\n",
    "                              compute_tda_on_the_fly=(not PRECOMPUTE_TDA),\n",
    "                              tda_out_size=TDA_OUT_SIZE, precompute_dir=TDA_DIR,\n",
    "                              transforms=transforms)\n",
    "\n",
    "    ds_test = AptosTDA_Dataset(test_df, DATA_DIR_TEST, img_size=IMG_SIZE,\n",
    "                               compute_tda_on_the_fly=(not PRECOMPUTE_TDA),\n",
    "                               tda_out_size=TDA_OUT_SIZE, precompute_dir=TDA_DIR,\n",
    "                               transforms=transforms)\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                              num_workers=NUM_WORKERS, pin_memory=(DEVICE==\"cuda\"))\n",
    "\n",
    "    val_loader = DataLoader(ds_val, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                            num_workers=NUM_WORKERS, pin_memory=(DEVICE==\"cuda\"))\n",
    "\n",
    "    test_loader = DataLoader(ds_test, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                             num_workers=NUM_WORKERS, pin_memory=(DEVICE==\"cuda\"))\n",
    "\n",
    "    # Model\n",
    "    backbone = CNNBackboneSpatial(pretrained=True)\n",
    "    model = TDA_CNN_Attention_Model(backbone=backbone, num_classes=5).to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "    criterion_ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_val_qwk = -1\n",
    "    patience_counter = 0\n",
    "    best_val_acc = 0.0\n",
    "    best_path = \"best_tda_att_model_fixed.pth\"\n",
    "\n",
    "    # LR SCHEDULER (Warmup + Cosine)\n",
    "    warmup_epochs = 3\n",
    "    total_epochs = NUM_EPOCHS\n",
    "\n",
    "    def lr_lambda(current_epoch):\n",
    "        if current_epoch < warmup_epochs:\n",
    "            # Linear warmup from 0 â†’ 1\n",
    "            return float(current_epoch) / float(max(1, warmup_epochs))\n",
    "        # Cosine decay after warmup\n",
    "        progress = float(current_epoch - warmup_epochs) / float(max(1, total_epochs - warmup_epochs))\n",
    "        return 0.5 * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "\n",
    "    # TRAINING! \n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        print(f\"\\nEpoch {epoch}/{NUM_EPOCHS}\")\n",
    "\n",
    "        tr_loss, tr_ce, tr_mask, tr_acc = train_one_epoch(\n",
    "            model, train_loader, optimizer, criterion_ce, LAMBDA_MASK, DEVICE\n",
    "        )\n",
    "        val_loss, val_ce, val_mask, val_acc, val_qwk = validate_one_epoch(\n",
    "            model, val_loader, criterion_ce, LAMBDA_MASK, DEVICE\n",
    "        )\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Train: loss={tr_loss:.4f} acc={tr_acc:.4f}\")\n",
    "        print(f\"Val  : loss={val_loss:.4f} acc={val_acc:.4f} qwk={val_qwk:.4f}\")\n",
    "\n",
    "        # ---------------- SAVE BEST BASED ON QWK ----------------\n",
    "        if val_qwk > best_val_qwk:\n",
    "            best_val_qwk = val_qwk\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            patience_counter = 0\n",
    "            print(\"Saved BEST model (by QWK).\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break \n",
    "\n",
    "    print(\"\\nTraining finished.\")\n",
    "    print(\"Best validation accuracy:\", best_val_acc)\n",
    "\n",
    "    # ------------------ FINAL TEST EVAL ------------------\n",
    "    print(\"\\nLoading best model for FINAL TEST evaluation...\")\n",
    "    model.load_state_dict(torch.load(best_path, map_location=DEVICE))\n",
    "\n",
    "    test_acc, test_qwk = evaluate_test(model, test_loader, DEVICE)\n",
    "    print(\"\\n=======================================\")\n",
    "    print(f\" FINAL TEST ACCURACY: {test_acc:.4f} \")\n",
    "    print(f\" FINAL TEST QWK:      {test_qwk:.4f} \")\n",
    "    print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
